{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255ef610",
   "metadata": {},
   "source": [
    "## Basic Langchain Based Prompt with Auto Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e566dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Importing Necessary libraries\n",
    "import os\n",
    "import bs4\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87cf0b",
   "metadata": {},
   "source": [
    "1. Setting Ollama Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bda850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"llama2:latest\")\n",
    "llm = OllamaLLM(model=\"llama2:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4da74",
   "metadata": {},
   "source": [
    "2. Loading Documents based on URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dd8bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://en.wikipedia.org/wiki/India\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            # class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4739a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents= documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc3f03",
   "metadata": {},
   "source": [
    "3. Creatting Vector Databases using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86cb5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(texts, embedding= embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d27aa6",
   "metadata": {},
   "source": [
    "4. Creating Retrival Question and Answer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80e24e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745aa49f",
   "metadata": {},
   "source": [
    "5.1. Generating answer for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18321186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the official name India?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer: The official name of India is Bhārat Gaṇarājya (ISO).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ground Truth: The official name of India is Republic of India\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the official name India?\"\n",
    "ground_truth = \"The official name of India is Republic of India\"\n",
    "\n",
    "\n",
    "answer = qa_chain.invoke({\"query\":question})['result']\n",
    "print(f\"Question: {question}\\n\" + \"-\"*100)\n",
    "print(f\"Answer: {answer}\\n\" + \"-\"*100)\n",
    "print(f\"Ground Truth: {ground_truth}\\n\" + \"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e283a9a",
   "metadata": {},
   "source": [
    "6.1. Evaluating The Answer for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4da659d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "[{'results': 'CORRECT. The student answer accurately reflects the official name of India, which is \"Bhārat Gaṇarājya\" according to the ISO standard.'}]\n"
     ]
    }
   ],
   "source": [
    "# Creating Evaluation Chain\n",
    "llm_eval = OllamaLLM(model='llama2:latest')\n",
    "\n",
    "eval_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\", \"ground_truth\"],\n",
    "    template=\"\"\"\n",
    "            You are a helpful assistant that evaluates how well the given answer matches the ground truth.\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            Ground Truth: {ground_truth}\n",
    "\n",
    "            Please provide a score from 1 to 5, where 1 means the answer is completely irrelevant, and 5 means the answer is perfectly aligned with the ground truth.\n",
    "            Also, provide a short explanation for your score.\n",
    "\n",
    "            Score:\n",
    "            Explanation:\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Preparing evaluation data\n",
    "eval_examples = [{\n",
    "    \"query\": question,\n",
    "    \"answer\": answer,\n",
    "    \"result\": ground_truth\n",
    "    }]\n",
    "\n",
    "# Creating eval chain\n",
    "eval_chain = QAEvalChain.from_llm(\n",
    "    llm= llm_eval\n",
    ")\n",
    "\n",
    "# Prediction text\n",
    "predictions = [{'result': answer}]\n",
    "\n",
    "# Run the evaluation\n",
    "evaluated_result = eval_chain.evaluate(eval_examples, predictions=predictions)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(evaluated_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76801848",
   "metadata": {},
   "source": [
    "7.1. Evaluating Answer using ROUGE method for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bdc88cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      " rouge1:  Score(precision=0.5, recall=0.6666666666666666, fmeasure=0.5714285714285715) \n",
      "rouge2:  Score(precision=0.45454545454545453, recall=0.625, fmeasure=0.5263157894736842) \n",
      "rougeL:  Score(precision=0.5, recall=0.6666666666666666, fmeasure=0.5714285714285715)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = True)\n",
    "rouge_scores = scorer.score(ground_truth, answer)\n",
    "print(\"ROUGE Scores:\\n\", \"rouge1: \", rouge_scores['rouge1'], \"\\nrouge2: \", rouge_scores['rouge2'], \"\\nrougeL: \", rouge_scores['rougeL'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bbfbf",
   "metadata": {},
   "source": [
    "8.1. Evaluating Answer using BLUE method for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50ad4dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41722614486115056\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Calculate the BLEU score\n",
    "results = bleu.compute(predictions=[answer], references=[ground_truth])\n",
    "\n",
    "# Print the results\n",
    "print(results['bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfb466",
   "metadata": {},
   "source": [
    "5.2. Generating ANswer for question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f61540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many states India have?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer: Based on the given context, India has 29 states.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ground Truth: India have 29 states.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"How many states India have?\"\n",
    "ground_truth = \"India have 29 states.\"\n",
    "\n",
    "\n",
    "answer = qa_chain.invoke({\"query\":question})['result']\n",
    "print(f\"Question: {question}\\n\" + \"-\"*100)\n",
    "print(f\"Answer: {answer}\\n\" + \"-\"*100)\n",
    "print(f\"Ground Truth: {ground_truth}\\n\" + \"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12710ac",
   "metadata": {},
   "source": [
    "6.2. Evaluating The Answer for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e393e5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "[{'results': 'GRADE: CORRECT'}]\n"
     ]
    }
   ],
   "source": [
    "# Preparing evaluation data\n",
    "eval_examples = [{\n",
    "    \"query\": question,\n",
    "    \"answer\": answer,\n",
    "    \"result\": ground_truth\n",
    "    }]\n",
    "\n",
    "# Creating eval chain\n",
    "eval_chain = QAEvalChain.from_llm(\n",
    "    llm= llm_eval\n",
    ")\n",
    "\n",
    "# Prediction text\n",
    "predictions = [{'result': answer}]\n",
    "\n",
    "# Run the evaluation\n",
    "evaluated_result = eval_chain.evaluate(eval_examples, predictions=predictions)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(evaluated_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8d478",
   "metadata": {},
   "source": [
    "7.2. Evaluating Answer using ROUGE method for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96d10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      " rouge1:  Score(precision=0.3333333333333333, recall=0.75, fmeasure=0.46153846153846156) \n",
      "rouge2:  Score(precision=0.125, recall=0.3333333333333333, fmeasure=0.18181818181818182) \n",
      "rougeL:  Score(precision=0.3333333333333333, recall=0.75, fmeasure=0.46153846153846156)\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = True)\n",
    "rouge_scores = scorer.score(ground_truth, answer)\n",
    "print(\"ROUGE Scores:\\n\", \"rouge1: \", rouge_scores['rouge1'], \"\\nrouge2: \", rouge_scores['rouge2'], \"\\nrougeL: \", rouge_scores['rougeL'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc89a6b",
   "metadata": {},
   "source": [
    "8.2. Evaluating Answer using BLUE method for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5aec68c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Calculate the BLEU score\n",
    "results = bleu.compute(predictions=[answer], references=[ground_truth])\n",
    "\n",
    "# Print the results\n",
    "print(results['bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9e0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
